#region Vocab
Reinforcement learning
Supervised learning
Unsupervised learning
Genetic learning

Policy
Reward Function
Value Function
Model

Secondary Reinforcers
The Credit Assignment Problem
Markov Decision Process
Control Theory
Bellman equation

Calculas
Linear Algebra
Statistics
Probability
#endregion

#region Questions
What learning algorithms are able to scale well with many inputs or decisions?
#endregion

#region Quotes

Supervised learning creates its own Value function?

    The Law of Effect includes the two most important aspects of what we mean
    by trial-and-error learning. First, it is selectional, meaning that it involves
    trying alternatives and selecting among them by comparing their consequences.
    Second, it is associative, meaning that the alternatives found by selection are
    associated with particular situations. Natural selection in evolution is a prime
    example of a selectional process, but it is not associative. Supervised learning
    is associative, but not selectional. It is the combination of these two that is
    essential to the Law of Effect and to trial-and-error learning. Another way of
    saying this is that the Law of Effect is an elementary way of combining search
    and memory: search in the form of trying and selecting among many actions in
    each situation, and memory in the form of remembering what actions worked
    best, associating them with the situations in which they were best. Combining
    search and memory in this way is essential to reinforcement learning.      
    --Reinforcement Learning: An Introduction p18

    #endregion

#region Thoughts
Through genetic algorithms and supervised learning it is possible to evolve/learn reinforcement learning solutions without actually doing reinforcement learning.
#endregion
    
#region Difference between supervised learning and reinforcement learning

    Barto, A. G., Anandan, P. (1985). Pattern recognizing stochastic learning automata. IEEE Transactions on Systems, Man, and Cybernetics, 15:360-375.

    Barto, A. G., Sutton, R. S. (1981a). Goal seeking components for adaptive intelligence: An initial assessment. Technical Report AFWAL-TR-81-1070. Air Force Wright Aeronautical Laboratories/Avionics Laboratory, Wright-Patterson AFB, OH.

    Barto, A. G., Sutton, R. S. (1981b). Landmark learning: An illustration of associative search. Biological Cybernetics, 42:1-8.

    Barto, A. G., Sutton, R. S., Brouwer, P. S. (1981). Associative search network: A reinforcement learning associative memory. Biological Cybernetics, 40:201-211.

    Klopf, A. H. (1972). Brain function and adaptive systems|A heterostatic theory. Technical Report AFCRL-72-0164, Air Force Cambridge Research Laboratories, Bedford, MA. A summary appears in Proceedings of the International Conference on Systems, Man, and Cybernetics. IEEE Systems, Man, and Cybernetics Society, Dallas, TX, 1974.

    Klopf, A. H. (1975). A comparison of natural and artificial intelligence. SIGART Newsletter, 53:11-13.

    Klopf, A. H. (1982). The Hedonistic Neuron: A Theory of Memory, Learning, and Intelligence. Hemisphere, Washington, DC.

    Klopf, A. H. (1988). A neuronal model of classical conditioning. Psychobiology, 16:85-125.

#endregion