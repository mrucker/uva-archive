n = |observations|
p = |variables|

X(n,p)

scalar = a 
vector = (aka tuple) 1xn matrix
matrix = 

Commutative = changing the order of operands doesn't change the result (e.g. a+b = b+a)
Associative = changing the order of a sequence doesn't change the result (e.g. [a+b]+c = a+[b+c])

"A central problem in all statistical learning situations involves choosing the best method for a given application." (p12)

Y = f(X) + e (p16)

f(X) represents the "ideal" relationship between X and Y. For example, if X is time and Y the is velocity of falling object, then the ideal relationship is Y = f(X) = 9.81 * X. (p18)

There are two kinds of errors that can be captured in Y = f(x) + e. One is explicitly defined: e. It is called the irreducible error. The other is implicit: the deviation of our calculated f(X) from the platonic ideal. It is calle dthe reducible error because we can always improve f(X). (p18)

Reasons for calculating f(X)
    > Prediction: to easily or cheaply determine Y given X without having to make real life observations (e.g. how fast will my plan fall out of the sky if I remove its wings?)
    > Inference: to understand how the various variables, X, impact Y (e.g. x1 might decrease Y while x2 increases it Y = f(x1, x2) = x2 - x1). (p17-19)
    
fundamental relationships types between Y and X, i.e., f in Y = f(X)
    > Linear
    > non-linear

fundamentals methods for determining f(X)
    > Parameteric: pick a pattern or model and then simply figure which values to assign the paramaters of that model to optimize it. (The least squares is an example of the parametric method. First, we pick the model "linear". Then, we adjust the parameters to minimize the squared distances.) (p21-22) 
    > Non-Parametric: 